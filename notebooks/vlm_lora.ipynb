{"cells":[{"cell_type":"markdown","metadata":{"id":"_QhAEOMFG5p_"},"source":["# VLM Notebook 2: Fine-tuning Vision-Language Models with LoRA\n"]},{"cell_type":"markdown","metadata":{"id":"1jAZYfYmG5qB"},"source":["## 1. Introduction to LoRA\n","\n","### What is LoRA?\n","\n","**LoRA (Low-Rank Adaptation)** is a parameter-efficient fine-tuning method that:\n","\n","- Freezes the pre-trained model weights\n","- Injects trainable low-rank decomposition matrices into each layer\n","- Reduces trainable parameters by 10,000x while maintaining performance\n","- Enables fine-tuning on consumer GPUs\n","\n","### Mathematical Foundation\n","\n","For a pre-trained weight matrix $W_0 \\in \\mathbb{R}^{d \\times k}$:\n","\n","$$W = W_0 + \\Delta W = W_0 + BA$$\n","\n","Where:\n","- $B \\in \\mathbb{R}^{d \\times r}$ and $A \\in \\mathbb{R}^{r \\times k}$\n","- $r \\ll \\min(d, k)$ (rank is much smaller)\n","- Only $B$ and $A$ are trained\n","\n","### Why LoRA for VLMs?\n","\n","| Aspect | Full Fine-tuning | LoRA |\n","|--------|-----------------|------|\n","| **Parameters** | Billions | Millions |\n","| **GPU Memory** | 80GB+ | 16GB |\n","| **Training Time** | Days | Hours |\n","| **Storage** | GBs per task | MBs per task |\n","| **Performance** | 100% | 95-99% |\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"T1e3RHtxG5qB"},"source":["## 2. Setup and Installation\n","\n","### Install Required Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yXHUS53lG5qB"},"outputs":[],"source":["!pip install -q -U trl bitsandbytes peft datasets tensorboard\n","!pip install -q transformers accelerate pillow requests matplotlib\n","!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"]},{"cell_type":"markdown","metadata":{"id":"eIsxuEzSG5qC"},"source":["### Import Dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eoFN4QNZG5qC"},"outputs":[],"source":["import torch\n","import numpy as np\n","from PIL import Image\n","import requests\n","from io import BytesIO\n","import matplotlib.pyplot as plt\n","from tqdm.auto import tqdm\n","import os\n","os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\n","# Transformers and PEFT\n","from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n","from peft import LoraConfig\n","\n","# Datasets\n","from datasets import load_dataset\n","\n","# TRL for SFTTrainer\n","from trl import SFTTrainer, SFTConfig\n","\n","# Set random seed for reproducibility\n","torch.manual_seed(42)\n","np.random.seed(42)\n","\n","# Check GPU availability\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","if torch.cuda.is_available():\n","    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n","    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"]},{"cell_type":"markdown","metadata":{"id":"vuFH2PT9G5qC"},"source":["---\n","\n","## 3. Dataset Preparation\n","\n","\n","### Load Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nzjGNSzuG5qC"},"outputs":[],"source":["from collections import Counter\n","import random\n","# from sklearn.model_selection import train_test_split # Removed sklearn's train_test_split\n","\n","# Load\n","dataset = load_dataset(\"lmms-lab/VQAv2\", split=\"validation[:1%]\")\n","\n","# Helper to get most frequent answer\n","def get_best_answer(example):\n","    answers = [ans[\"answer\"] for ans in example[\"answers\"]]\n","    return Counter(answers).most_common(1)[0][0]\n","\n","# Format as chat: user (image + question) → assistant (answer)\n","def format_chat(example):\n","    best_answer = get_best_answer(example)\n","    messages = [\n","        {\n","            \"role\": \"user\",\n","            \"content\": [\n","                {\"type\": \"image\"},\n","                {\"type\": \"text\", \"text\": example[\"question\"]}\n","            ]\n","        },\n","        {\n","            \"role\": \"assistant\",\n","            \"content\": [{\"type\": \"text\", \"text\": best_answer}]\n","        }\n","    ]\n","    # Return messages and the original image column to keep it in the dataset\n","    return {\"messages\": messages, \"image\": example[\"image\"]}\n","\n","# Apply formatting\n","formatted_dataset = dataset.map(format_chat, remove_columns=dataset.column_names)\n","\n","# Split for train/eval (80/20) using the datasets library's own train_test_split method\n","split_dataset = formatted_dataset.train_test_split(test_size=0.2, seed=42)\n","train_dataset = split_dataset[\"train\"]\n","eval_dataset = split_dataset[\"test\"]\n","\n","print(f\"Training samples: {len(train_dataset)}\")\n","print(f\"Validation samples: {len(eval_dataset)}\")\n","print(f\"Sample train data keys: {train_dataset[0].keys()}\")"]},{"cell_type":"markdown","metadata":{"id":"0xTX7t9zG5qD"},"source":["### Visualize Dataset Samples"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MfmbQaRNG5qD"},"outputs":[],"source":["# Take the first example\n","sample = dataset[0]\n","\n","# Display the image\n","image = sample[\"image\"]  # This is already a PIL Image in lmms-lab/VQAv2\n","plt.figure(figsize=(8, 8))\n","plt.imshow(image)\n","plt.axis(\"off\")\n","plt.title(f\"Question: {sample['question']}\", fontsize=14, pad=20)\n","plt.show()\n","\n","# Print question and all answers with their counts\n","print(f\"Question: {sample['question']}\")\n","print(f\"Question ID: {sample['question_id']}\")\n","print(\"\\nAnswers:\")\n","for answer in sample[\"answers\"]:\n","    print(f\"  • {answer['answer']}\")"]},{"cell_type":"markdown","metadata":{"id":"Vn8wQnVmG5qE"},"source":["---\n","\n","## 4. Model Preparation\n","\n","### Load Model and Processor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7HOzj2b5G5qE"},"outputs":[],"source":["model_id = \"google/gemma-3-4b-it\"  # Smallest multimodal variant\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.bfloat16,\n","    bnb_4bit_quant_storage=torch.bfloat16,\n",")\n","\n","model = AutoModelForImageTextToText.from_pretrained(\n","    model_id,\n","    device_map=\"auto\",\n","    dtype=torch.bfloat16,\n","    attn_implementation=\"eager\",\n","    quantization_config=bnb_config\n",")\n","\n","processor = AutoProcessor.from_pretrained(model_id)\n","processor.tokenizer.padding_side = \"right\""]},{"cell_type":"markdown","metadata":{"id":"sryi665NG5qE"},"source":["### Configure LoRA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"165E8V1XG5qE"},"outputs":[],"source":["# LoRA configuration\n","peft_config = LoraConfig(\n","    lora_alpha=16,\n","    lora_dropout=0.05,\n","    r=16,\n","    bias=\"none\",\n","    target_modules=\"all-linear\",\n","    task_type=\"CAUSAL_LM\",\n","    modules_to_save=[\"lm_head\", \"embed_tokens\"],\n","    ensure_weight_tying=True,\n",")"]},{"cell_type":"markdown","metadata":{"id":"mKzWoPRMG5qE"},"source":["### Understanding LoRA Parameters\n","\n","| Parameter | Description | Typical Values |\n","|-----------|-------------|----------------|\n","| **r** | Rank of adaptation matrices | 8, 16, 32, 64 |\n","| **lora_alpha** | Scaling factor (typically 2×r) | 16, 32, 64 |\n","| **target_modules** | Which layers to adapt | q_proj, v_proj, k_proj |\n","| **lora_dropout** | Regularization | 0.05, 0.1 |\n","\n","**Trade-offs:**\n","- Higher `r` → More capacity but more parameters\n","- More `target_modules` → Better performance but slower training"]},{"cell_type":"markdown","metadata":{"id":"GaWu-bT0G5qE"},"source":["---\n","\n","## 5. Training Setup\n","### Define Collate Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_OCwIPHhG5qF"},"outputs":[],"source":["def collate_fn(examples):\n","    texts = []\n","    images = []\n","\n","    for ex in examples:\n","        # Apply chat template to full conversation (includes assistant for SFT)\n","        text = processor.apply_chat_template(\n","            ex[\"messages\"],\n","            tokenize=False,\n","            add_generation_prompt=False,  # No extra prompt for training\n","        ).strip()\n","        texts.append(text)\n","        images.append(ex[\"image\"].convert(\"RGB\"))\n","\n","    # Processor tokenizes + embeds images\n","    batch = processor(\n","        text=texts,\n","        images=images,\n","        return_tensors=\"pt\",\n","        padding=True,\n","        truncation=True,\n","        max_length=512,  # Tune based on your VRAM; Gemma supports 128K\n","    )\n","\n","    # Labels: Clone input_ids, mask pads (image embeddings auto-ignored in loss)\n","    labels = batch[\"input_ids\"].clone()\n","    labels[labels == processor.tokenizer.pad_token_id] = -100\n","\n","    batch[\"labels\"] = labels\n","    return batch"]},{"cell_type":"markdown","metadata":{"id":"IhDSb2K4G5qF"},"source":["### Configure Training Arguments"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tzWhbnFvG5qF"},"outputs":[],"source":["training_args = SFTConfig(\n","    output_dir=\"gemma-3-4b-vqa-finetuned\",\n","    num_train_epochs=1,\n","    per_device_train_batch_size=1,\n","    gradient_accumulation_steps=4,\n","    gradient_checkpointing=True,\n","    optim=\"adamw_torch_fused\",\n","    save_strategy=\"epoch\",\n","    learning_rate=2e-05,\n","    bf16=True,\n","    report_to=\"none\",\n","    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n","    dataset_kwargs={\"skip_prepare_dataset\": True},\n","    remove_unused_columns=False,\n","    logging_steps=10,  # For monitoring\n",")"]},{"cell_type":"markdown","source":["### Set Up SFTTrainer"],"metadata":{"id":"6aafY9dziW8Y"}},{"cell_type":"code","source":["trainer = SFTTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n","    data_collator=collate_fn,\n","    peft_config=peft_config,\n","    processing_class=processor,\n",")"],"metadata":{"id":"lFaOsYEeiZw5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","## 6. Train the Model"],"metadata":{"id":"8OU3DHt8ib32"}},{"cell_type":"code","source":["trainer.train()"],"metadata":{"id":"5EEKaCjJibSu"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"},"colab":{"provenance":[],"gpuType":"A100"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}
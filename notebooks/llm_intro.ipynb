{"cells":[{"cell_type":"markdown","id":"fffa426b","metadata":{"id":"fffa426b"},"source":["\n","# LLM Intro: From Tokens to Transformer Forward Pass\n","\n","This notebook accompanies the **Introduction to LLMs** module. Run cells top-to-bottom.\n","It mirrors the sections:\n","1. What is an LLM?\n","2. From Words to Tokens\n","3. Transformer Architecture\n","4. Positional Encoding\n","5. Putting It All Together\n","\n","> **Tip:** If you're on Colab, make sure the runtime is set to Python 3.10+.\n"]},{"cell_type":"markdown","id":"45bd3af5","metadata":{"id":"45bd3af5"},"source":["\n","## 0. Environment & Imports\n","\n","We'll use PyTorch and (optionally) Hugging Face Tokenizers/Transformers.\n","If you're on Colab, the first `pip` cell will fetch dependencies.\n"]},{"cell_type":"code","execution_count":null,"id":"7b651c7b","metadata":{"id":"7b651c7b"},"outputs":[],"source":["\n","# If running on Colab, uncomment to install latest versions:\n","# !pip -q install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n","# !pip -q install transformers tokenizers\n"]},{"cell_type":"code","execution_count":null,"id":"00a42fe3","metadata":{"id":"00a42fe3"},"outputs":[],"source":["\n","import math, os, sys, json, random, time\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# Matplotlib for a few simple visualizations\n","import matplotlib.pyplot as plt\n","\n","print(f'Torch: {torch.__version__}, CUDA available: {torch.cuda.is_available()}')\n"]},{"cell_type":"markdown","id":"9530ba27","metadata":{"id":"9530ba27"},"source":["\n","## 1. What is a Large Language Model? (Quick Code Peek)\n","\n","Language models learn the conditional probability of the next token given previous tokens.\n","Below we define a helper to apply softmax to logits and inspect the distribution.\n"]},{"cell_type":"code","execution_count":null,"id":"20233047","metadata":{"id":"20233047"},"outputs":[],"source":["\n","import torch\n","\n","def softmax(logits):\n","    exps = torch.exp(logits - logits.max())  # numerical stability\n","    return exps / exps.sum(dim=-1, keepdim=True)\n","\n","logits = torch.tensor([2.0, 1.0, 0.5])\n","probs = softmax(logits)\n","print('Logits:', logits.tolist())\n","print('Probs:', probs.tolist(), 'sum=', float(probs.sum()))\n","print('Argmax token index:', int(probs.argmax()))\n"]},{"cell_type":"markdown","id":"72ec1dd1","metadata":{"id":"72ec1dd1"},"source":["\n","## 2. From Words to Tokens\n","\n","We'll demonstrate tokenization using GPT-2's tokenizer from Hugging Face.\n"]},{"cell_type":"code","execution_count":null,"id":"92a4c14c","metadata":{"id":"92a4c14c"},"outputs":[],"source":["\n","from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n","text = \"Transformers are powerful models.\"\n","tokens = tokenizer.tokenize(text)\n","token_ids = tokenizer.encode(text)\n","print('Tokens:', tokens)\n","print('Token IDs:', token_ids)\n"]},{"cell_type":"code","execution_count":null,"id":"4dd5a04b","metadata":{"id":"4dd5a04b"},"outputs":[],"source":["\n","# Embedding lookup demo (toy dimensions)\n","V, d = 50000, 8\n","embedding = nn.Embedding(V, d)\n","\n","# Map token IDs to embeddings (note: GPT-2 vocab ~50k, so IDs fit this toy V)\n","ids = torch.tensor(token_ids)\n","X = embedding(ids)\n","print('Embedding shape:', tuple(X.shape))  # (T, d)\n"]},{"cell_type":"markdown","id":"42eae369","metadata":{"id":"42eae369"},"source":["\n","## 3. Transformer Architecture: Self-Attention & Feedforward\n","\n","We'll implement one attention step by hand, then use PyTorch's built-in MultiheadAttention.\n"]},{"cell_type":"code","execution_count":null,"id":"9fad5657","metadata":{"id":"9fad5657"},"outputs":[],"source":["\n","# Manual single-head attention on small tensors\n","T, d = 4, 8\n","X = torch.rand(T, d)\n","W_Q = torch.rand(d, d)\n","W_K = torch.rand(d, d)\n","W_V = torch.rand(d, d)\n","\n","Q = X @ W_Q\n","K = X @ W_K\n","V = X @ W_V\n","\n","A = F.softmax(Q @ K.T / (d ** 0.5), dim=-1)  # attention weights\n","Z = A @ V\n","\n","print('Attention weights shape:', tuple(A.shape))  # (T, T)\n","print('Context shape:', tuple(Z.shape))           # (T, d)\n"]},{"cell_type":"code","execution_count":null,"id":"5c01e915","metadata":{"id":"5c01e915"},"outputs":[],"source":["\n","# Multi-head attention using nn.MultiheadAttention\n","mha = nn.MultiheadAttention(embed_dim=8, num_heads=2, batch_first=True)\n","X_batched = torch.rand(1, 4, 8)  # (batch, tokens, dim)\n","out, weights = mha(X_batched, X_batched, X_batched)\n","print('MHA out shape:', tuple(out.shape))        # (1, 4, 8)\n","print('MHA weights shape:', tuple(weights.shape))# (1, 2, 4, 4) = (batch, heads, T, T)\n"]},{"cell_type":"code","execution_count":null,"id":"f7b3e79c","metadata":{"id":"f7b3e79c"},"outputs":[],"source":["\n","# Feedforward network (position-wise MLP)\n","ffn = nn.Sequential(\n","    nn.Linear(8, 16),\n","    nn.ReLU(),\n","    nn.Linear(16, 8)\n",")\n","X_ffn = ffn(out)\n","print('FFN out shape:', tuple(X_ffn.shape))\n"]},{"cell_type":"markdown","id":"416b4694","metadata":{"id":"416b4694"},"source":["\n","## 4. Positional Encoding (Sinusoidal)\n","\n","We'll create sinusoidal positional encodings and visualize a few dimensions across positions.\n"]},{"cell_type":"code","execution_count":null,"id":"f359b3e9","metadata":{"id":"f359b3e9"},"outputs":[],"source":["\n","def positional_encoding(max_len, d_model):\n","    PE = torch.zeros(max_len, d_model)\n","    for pos in range(max_len):\n","        for i in range(0, d_model, 2):\n","            angle = pos / (10000 ** (2 * i / d_model))\n","            PE[pos, i] = math.sin(angle)\n","            if i + 1 < d_model:\n","                PE[pos, i + 1] = math.cos(angle)\n","    return PE\n","\n","PE = positional_encoding(max_len=32, d_model=8)\n","print('PE shape:', tuple(PE.shape))\n"]},{"cell_type":"code","execution_count":null,"id":"dd24fec0","metadata":{"id":"dd24fec0"},"outputs":[],"source":["\n","# Plot positional encoding patterns for a few dimensions\n","plt.figure(figsize=(8, 3))\n","plt.plot(PE[:, 0], label='dim 0 (sin)')\n","plt.plot(PE[:, 1], label='dim 1 (cos)')\n","plt.plot(PE[:, 2], label='dim 2 (sin)')\n","plt.xlabel('Token position')\n","plt.ylabel('Encoding value')\n","plt.title('Example Positional Encoding Patterns')\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"markdown","id":"e1ba1bbb","metadata":{"id":"e1ba1bbb"},"source":["\n","## 5. Putting It All Together: A Minimal Transformer Layer\n","\n","We now assemble a single Transformer block with MHA, FFN, and LayerNorm, then run a forward pass.\n"]},{"cell_type":"code","execution_count":null,"id":"bbb553c6","metadata":{"id":"bbb553c6"},"outputs":[],"source":["\n","class MiniTransformerLayer(nn.Module):\n","    def __init__(self, d_model=8, nhead=2, dim_ff=16):\n","        super().__init__()\n","        self.mha = nn.MultiheadAttention(d_model, nhead, batch_first=True)\n","        self.ffn = nn.Sequential(\n","            nn.Linear(d_model, dim_ff),\n","            nn.ReLU(),\n","            nn.Linear(dim_ff, d_model)\n","        )\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.norm2 = nn.LayerNorm(d_model)\n","\n","    def forward(self, X):\n","        attn_out, attn_w = self.mha(X, X, X)\n","        X = self.norm1(X + attn_out)\n","        ffn_out = self.ffn(X)\n","        X = self.norm2(X + ffn_out)\n","        return X, attn_w\n","\n","torch.manual_seed(0)\n","X = torch.rand(1, 5, 8)  # (batch, tokens, dim)\n","layer = MiniTransformerLayer()\n","out, attn_w = layer(X)\n","print('Output shape:', tuple(out.shape))\n","print('Attention weights shape:', tuple(attn_w.shape))\n"]},{"cell_type":"code","execution_count":null,"id":"902e14f3","metadata":{"id":"902e14f3"},"outputs":[],"source":["# Visualize attention weights for head 0\n","# Note: weights shape is (batch, T, T), we'll plot for batch=0\n","w = attn_w[0].detach()\n","plt.figure(figsize=(4, 3))\n","plt.imshow(w, aspect='auto')\n","plt.colorbar()\n","plt.title('Attention Weights (head 0)')\n","plt.xlabel('Key positions')\n","plt.ylabel('Query positions')\n","plt.show()"]},{"cell_type":"markdown","id":"b91fcf7f","metadata":{"id":"b91fcf7f"},"source":["\n","## 6. Output Projection & Softmax (Toy Demo)\n","\n","Finally, we project the hidden state at each position to vocabulary logits and apply softmax.\n"]},{"cell_type":"code","execution_count":null,"id":"47a0b6be","metadata":{"id":"47a0b6be"},"outputs":[],"source":["\n","# Toy vocabulary projection\n","V = 1000\n","d_model = out.shape[-1]\n","Wo = nn.Linear(d_model, V)\n","\n","logits = Wo(out)             # (batch, T, V)\n","probs = F.softmax(logits, dim=-1)\n","print('Logits shape:', tuple(logits.shape))\n","print('Probs shape:', tuple(probs.shape), 'Row sums (approx 1):', probs[0,0,:999].sum().item())\n"]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}
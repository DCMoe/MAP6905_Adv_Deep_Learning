{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6f3b6897",
      "metadata": {
        "id": "6f3b6897"
      },
      "source": [
        "\n",
        "# Mini Transformer Training: Cross-Entropy, Perplexity, and Attention\n",
        "\n",
        "This notebook accompanies **Module 2: Transformer Math & Training Objectives**.\n",
        "\n",
        "**What you'll do:**\n",
        "- Load a tiny text dataset from Hugging Face\n",
        "- Tokenize with GPT-2 tokenizer\n",
        "- Train a small Transformer (2 layers, 2 heads) on next-token prediction\n",
        "- Plot the **loss curve**\n",
        "- Compute **perplexity**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "032295d6",
      "metadata": {
        "id": "032295d6"
      },
      "source": [
        "\n",
        "## 0. Setup & Installs\n",
        "If you're on Google Colab, run the following to install dependencies. Then **Restart Runtime** if prompted.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35f86e0f",
      "metadata": {
        "id": "35f86e0f"
      },
      "outputs": [],
      "source": [
        "\n",
        "# If running on Colab, uncomment to install/upgrade dependencies:\n",
        "# !pip -q install datasets transformers torch matplotlib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d32705ba",
      "metadata": {
        "id": "d32705ba"
      },
      "source": [
        "## 1. Imports & Reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73cea3b2",
      "metadata": {
        "id": "73cea3b2"
      },
      "outputs": [],
      "source": [
        "\n",
        "import math, random, os, sys, time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Reproducibility\n",
        "seed = 0\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Torch:\", torch.__version__, \"| CUDA:\", torch.cuda.is_available(), \"| Device:\", device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b85393b",
      "metadata": {
        "id": "8b85393b"
      },
      "source": [
        "\n",
        "## 2. Load a Tiny Text Dataset\n",
        "\n",
        "We'll use **WikiText-2 (raw)** and only a small subset to keep things fast.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d7d938b",
      "metadata": {
        "id": "6d7d938b"
      },
      "outputs": [],
      "source": [
        "\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train[:1%]\")\n",
        "print(dataset[0][\"text\"][:200].replace(\"\\n\", \" \"))\n",
        "print(\"Num samples:\", len(dataset))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a2a195f",
      "metadata": {
        "id": "0a2a195f"
      },
      "source": [
        "## 3. Tokenize with GPT-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e2e6ced",
      "metadata": {
        "id": "2e2e6ced"
      },
      "outputs": [],
      "source": [
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "# GPT-2 doesn't have a pad token by default; set eos_token as pad for batching simplicity\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "MAX_LEN = 64\n",
        "\n",
        "def tokenize_fn(batch):\n",
        "    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LEN)\n",
        "\n",
        "tokenized = dataset.map(tokenize_fn, batched=True, remove_columns=dataset.column_names)\n",
        "input_ids = torch.tensor(tokenized[\"input_ids\"])[:256]  # small subset\n",
        "attn_mask = torch.tensor(tokenized[\"attention_mask\"])[:256]\n",
        "print(\"input_ids shape:\", tuple(input_ids.shape), \"| attention_mask shape:\", tuple(attn_mask.shape))\n",
        "vocab_size = tokenizer.vocab_size\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b050ef9e",
      "metadata": {
        "id": "b050ef9e"
      },
      "source": [
        "## 4. Create Mini Batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e91ad7e",
      "metadata": {
        "id": "8e91ad7e"
      },
      "outputs": [],
      "source": [
        "\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "def iterate_batches(x, m, batch_size=BATCH_SIZE):\n",
        "    for i in range(0, x.size(0), batch_size):\n",
        "        yield x[i:i+batch_size], m[i:i+batch_size]\n",
        "\n",
        "print(\"Example first batch sizes:\", next(iterate_batches(input_ids, attn_mask))[0].shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abd05f96",
      "metadata": {
        "id": "abd05f96"
      },
      "source": [
        "## 5. Define a Tiny Transformer (with positional embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1849da9d",
      "metadata": {
        "id": "1849da9d"
      },
      "outputs": [],
      "source": [
        "\n",
        "class TinyTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=128, nhead=2, nlayers=2, max_len=MAX_LEN, pad_id=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos = nn.Embedding(max_len, d_model)\n",
        "        layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
        "        self.encoder = nn.TransformerEncoder(layer, num_layers=nlayers)\n",
        "        self.decoder = nn.Linear(d_model, vocab_size)\n",
        "        self.pad_id = pad_id\n",
        "\n",
        "    def causal_mask(self, T, device):\n",
        "        # True = mask out (prevent attending to future)\n",
        "        mask = torch.triu(torch.ones(T, T, device=device, dtype=torch.bool), diagonal=1)\n",
        "        return mask\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T = x.shape\n",
        "        pos = torch.arange(T, device=x.device)\n",
        "        h = self.embed(x) + self.pos(pos)\n",
        "        # causal mask ensures token t cannot attend to future tokens > t\n",
        "        mask = self.causal_mask(T, x.device)\n",
        "        out = self.encoder(h, mask=mask)\n",
        "        logits = self.decoder(out)\n",
        "        return logits, out  # return logits and hidden states\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a50134b2",
      "metadata": {
        "id": "a50134b2"
      },
      "source": [
        "## 6. Train the Model and Plot Loss Curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "896fe3c9",
      "metadata": {
        "id": "896fe3c9"
      },
      "outputs": [],
      "source": [
        "\n",
        "from torch.optim import AdamW\n",
        "\n",
        "model = TinyTransformer(vocab_size=vocab_size).to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-3)\n",
        "EPOCHS = 5\n",
        "\n",
        "losses = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    n_tokens = 0\n",
        "    for xb, mb in iterate_batches(input_ids, attn_mask):\n",
        "        xb = xb.to(device)\n",
        "        logits, _ = model(xb)\n",
        "        # Next-token prediction: shift targets by 1\n",
        "        targets = xb[:, 1:].contiguous()\n",
        "        logits_flat = logits[:, :-1, :].contiguous().view(-1, vocab_size)\n",
        "        targets_flat = targets.view(-1)\n",
        "\n",
        "        loss = F.cross_entropy(logits_flat, targets_flat, ignore_index=tokenizer.pad_token_id)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += float(loss.item()) * targets_flat.numel()\n",
        "        n_tokens += (targets_flat != tokenizer.pad_token_id).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / max(n_tokens, 1)\n",
        "    losses.append(avg_loss)\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} - Avg token loss: {avg_loss:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.plot(losses, marker='o')\n",
        "plt.title(\"Training Loss Curve\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Average Token Loss\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75d017aa",
      "metadata": {
        "id": "75d017aa"
      },
      "source": [
        "## 7. Evaluate Perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41f25b92",
      "metadata": {
        "id": "41f25b92"
      },
      "outputs": [],
      "source": [
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    xb = input_ids[:32].to(device)\n",
        "    logits, _ = model(xb)\n",
        "    targets = xb[:, 1:].contiguous()\n",
        "    logits_flat = logits[:, :-1, :].contiguous().view(-1, vocab_size)\n",
        "    targets_flat = targets.view(-1)\n",
        "    loss = F.cross_entropy(logits_flat, targets_flat, ignore_index=tokenizer.pad_token_id)\n",
        "    ppl = torch.exp(loss)\n",
        "    print(f\"Validation loss: {loss:.4f} | Perplexity: {ppl:.2f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
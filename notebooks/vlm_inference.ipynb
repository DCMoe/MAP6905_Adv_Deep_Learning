{"cells":[{"cell_type":"markdown","metadata":{"id":"gXqIwuA_pXmB"},"source":["# VLM Notebook 1: Using Pre-trained Vision-Language Models"]},{"cell_type":"markdown","metadata":{"id":"1swD6f2YpXmD"},"source":["## 1. Environment Setup\n","\n","First, let's install the required libraries and verify GPU availability."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sLyeHCW2pXmD"},"outputs":[],"source":["# Install required packages\n","!pip install -q transformers accelerate pillow requests torch torchvision\n","!pip install -q bitsandbytes  # For efficient loading\n","\n","print(\"✓ All packages installed successfully!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6MCpHVNtpXmE"},"outputs":[],"source":["# Import libraries\n","import torch\n","import requests\n","from PIL import Image\n","from io import BytesIO\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from transformers import (\n","    AutoProcessor,\n","    AutoModelForCausalLM,\n","    Blip2Processor,\n","    Blip2ForConditionalGeneration,\n","    CLIPProcessor,\n","    CLIPModel,\n",")\n","\n","# Check GPU availability\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using device: {device}\")\n","\n","if device == \"cuda\":\n","    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n","    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n","else:\n","    print(\"⚠️ Warning: Running on CPU. Inference will be slow.\")"]},{"cell_type":"markdown","metadata":{"id":"GE2dkztUpXmF"},"source":["### Helper Functions\n","\n","Let's create utility functions for loading and displaying images."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lDVixkhopXmF"},"outputs":[],"source":["def load_image(image_path_or_url):\n","    \"\"\"\n","    Load an image from a local path or URL.\n","\n","    Args:\n","        image_path_or_url: Local file path or HTTP(S) URL\n","\n","    Returns:\n","        PIL Image object\n","    \"\"\"\n","    if image_path_or_url.startswith(('http://', 'https://')):\n","        response = requests.get(image_path_or_url)\n","        image = Image.open(BytesIO(response.content)).convert('RGB')\n","    else:\n","        image = Image.open(image_path_or_url).convert('RGB')\n","    return image\n","\n","def display_image(image, title=None, figsize=(8, 6)):\n","    \"\"\"\n","    Display an image with optional title.\n","\n","    Args:\n","        image: PIL Image object\n","        title: Optional title string\n","        figsize: Figure size tuple (width, height)\n","    \"\"\"\n","    plt.figure(figsize=figsize)\n","    plt.imshow(image)\n","    plt.axis('off')\n","    if title:\n","        plt.title(title, fontsize=14, fontweight='bold')\n","    plt.show()\n","\n","print(\"✓ Helper functions defined\")"]},{"cell_type":"markdown","metadata":{"id":"SuEHGJi0pXmF"},"source":["### Load Sample Images\n","\n","We'll use some example images from the web for demonstration."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KLG3CIxppXmF"},"outputs":[],"source":["from datasets import load_dataset\n","\n","print(\"Loading images from microsoft/cats_vs_dogs dataset...\")\n","\n","# Load dataset in streaming mode to avoid downloading the entire dataset\n","dataset = load_dataset(\"microsoft/cats_vs_dogs\", split=\"train\", streaming=True)\n","\n","images = {}\n","\n","# Iterate through the dataset to find one cat and one dog\n","# Labels: 0 = Cat, 1 = Dog\n","for example in dataset:\n","    label = example['labels']\n","    if 'cat' not in images and label == 0:\n","        images['cat'] = example['image'].convert(\"RGB\")\n","        print(\"✓ Loaded cat image from dataset\")\n","    elif 'dog' not in images and label == 1:\n","        images['dog'] = example['image'].convert(\"RGB\")\n","        print(\"✓ Loaded dog image from dataset\")\n","\n","    if 'cat' in images and 'dog' in images:\n","        break\n","\n","# Display sample images\n","if len(images) > 0:\n","    fig, axes = plt.subplots(1, len(images), figsize=(15, 5))\n","    if len(images) == 1:\n","        axes = [axes]\n","    for ax, (name, img) in zip(axes, images.items()):\n","        ax.imshow(img)\n","        ax.set_title(name.capitalize(), fontsize=12, fontweight='bold')\n","        ax.axis('off')\n","    plt.tight_layout()\n","    plt.show()\n","else:\n","    print(\"No images loaded to display.\")"]},{"cell_type":"markdown","metadata":{"id":"D-ccft3ApXmF"},"source":["## 2. Image Captioning with BLIP-2\n","\n","BLIP-2 is a model for image captioning that uses a lightweight Q-Former to bridge vision and language.\n","\n","**Key Concepts (from Module 2):**\n","- Q-Former extracts visual features using learnable queries\n","- Queries act as information bottleneck\n","- Connected to frozen LLM for generation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T7MNp3TapXmG"},"outputs":[],"source":["# Load BLIP-2 model\n","print(\"Loading BLIP-2 model... (this may take a few minutes)\")\n","\n","blip_processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n","blip_model = Blip2ForConditionalGeneration.from_pretrained(\n","    \"Salesforce/blip2-opt-2.7b\",\n","    torch_dtype=torch.float16,  # Use FP16 for efficiency\n","    device_map=\"auto\"  # Automatically map to available devices\n",")\n","\n","print(\"✓ BLIP-2 model loaded successfully\")\n","print(f\"Model size: {sum(p.numel() for p in blip_model.parameters()) / 1e9:.2f}B parameters\")"]},{"cell_type":"markdown","metadata":{"id":"AcRVjOcPpXmG"},"source":["### Generate Captions with Different Decoding Strategies"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x4ty42SzpXmG"},"outputs":[],"source":["def generate_caption(image, decoding_strategy=\"greedy\", max_length=50):\n","    \"\"\"\n","    Generate caption for an image using BLIP-2.\n","\n","    Args:\n","        image: PIL Image\n","        decoding_strategy: One of [\"greedy\", \"beam_search\", \"nucleus\"]\n","        max_length: Maximum caption length\n","\n","    Returns:\n","        Generated caption string\n","    \"\"\"\n","    # Process image\n","    inputs = blip_processor(images=image, return_tensors=\"pt\").to(device, torch.float16)\n","\n","    # Generate based on strategy\n","    if decoding_strategy == \"greedy\":\n","        # Greedy: Select most probable token at each step\n","        generated_ids = blip_model.generate(\n","            **inputs,\n","            max_length=max_length,\n","            do_sample=False  # Deterministic\n","        )\n","    elif decoding_strategy == \"beam_search\":\n","        # Beam search: Keep top-k candidates\n","        generated_ids = blip_model.generate(\n","            **inputs,\n","            max_length=max_length,\n","            num_beams=5,  # Number of beams\n","            do_sample=False\n","        )\n","    elif decoding_strategy == \"nucleus\":\n","        # Nucleus (top-p) sampling: Sample from top cumulative probability\n","        generated_ids = blip_model.generate(\n","            **inputs,\n","            max_length=max_length,\n","            do_sample=True,\n","            top_p=0.9,  # Nucleus probability\n","            temperature=1.0  # Sampling temperature\n","        )\n","    else:\n","        raise ValueError(f\"Unknown strategy: {decoding_strategy}\")\n","\n","    # Decode to text\n","    caption = blip_processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n","    return caption\n","\n","# Test different decoding strategies\n","test_image = images[\"dog\"]\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"CAPTION GENERATION COMPARISON\")\n","print(\"=\"*60 + \"\\n\")\n","\n","for strategy in [\"greedy\", \"beam_search\", \"nucleus\"]:\n","    caption = generate_caption(test_image, decoding_strategy=strategy)\n","    print(f\"{strategy.upper():15s}: {caption}\")\n","\n","display_image(test_image, \"Test Image\")"]},{"cell_type":"markdown","metadata":{"id":"LY2c10bEpXmG"},"source":["### Caption All Sample Images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m30XvNJTpXmG"},"outputs":[],"source":["# Generate captions for all images\n","captions = {}\n","\n","for name, image in images.items():\n","    caption = generate_caption(image, decoding_strategy=\"beam_search\")\n","    captions[name] = caption\n","    print(f\"\\n{name.upper()}:\")\n","    print(f\"  Caption: {caption}\")\n","\n","# Visualize images with captions\n","fig, axes = plt.subplots(1, len(images), figsize=(18, 6))\n","for ax, (name, img) in zip(axes, images.items()):\n","    ax.imshow(img)\n","    ax.set_title(f\"{name.capitalize()}\\n{captions[name]}\",\n","                 fontsize=10, wrap=True)\n","    ax.axis('off')\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"cemOKvB3pXmH"},"source":["### Single-Turn VQA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S0s8BYp9pXmH"},"outputs":[],"source":["def answer_question(image, question, max_new_tokens=20):\n","    \"\"\"\n","    Answer a question about an image using BLIP-2.\n","\n","    Args:\n","        image: PIL Image\n","        question: Question string\n","        max_new_tokens: Maximum tokens in answer\n","\n","    Returns:\n","        Answer string\n","    \"\"\"\n","    # Prepare prompt for VQA\n","    prompt = f\"Question: {question} Answer:\"\n","\n","    # Process inputs\n","    inputs = blip_processor(images=image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n","\n","    # Generate answer\n","    generated_ids = blip_model.generate(\n","        **inputs,\n","        max_new_tokens=max_new_tokens,\n","        do_sample=False\n","    )\n","\n","    # Decode output\n","    answer = blip_processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n","\n","    return answer\n","\n","# Test VQA on sample images\n","vqa_examples = [\n","    (\"dog\", \"What breed is this dog?\"),\n","    (\"dog\", \"Is the dog sitting or standing?\"),\n","    (\"cat\", \"What color is the cat?\"),\n","]\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"VISUAL QUESTION ANSWERING EXAMPLES (BLIP-2)\")\n","print(\"=\"*60 + \"\\n\")\n","\n","for image_name, question in vqa_examples:\n","    if image_name in images:\n","        image = images[image_name]\n","        answer = answer_question(image, question)\n","        print(f\"Image: {image_name.upper()}\")\n","        print(f\"Q: {question}\")\n","        print(f\"A: {answer}\")\n","        print()\n","    else:\n","        print(f\"Skipping {image_name}: Image not loaded.\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}
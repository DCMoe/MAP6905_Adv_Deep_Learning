@article{aghajanyan2021intrinsic,
  title={Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning},
  author={Aghajanyan, Armen and Gupta, Akshat and Shrivastava, Akshita and Chen, Xilun and Goyal, Luke and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2012.13255},
  year={2021}
}

@article{alayrac2022flamingo,
  title={Flamingo: a Visual Language Model for Few-Shot Learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Lucic, Marie and Miech, Antoine and Barr, Iain and Gokalp, Yana and Hill, Tejas and McKenna, Shida and Natser, Julia and Prangemeier, Tadek and others},
  journal={arXiv preprint arXiv:2204.14198},
  year={2022}
}

@misc{brown2020language,
  title={Language Models are Few-Shot Learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  year={2020},
  howpublished={\url{https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf}},
  note={NeurIPS 2020}
}

@article{choromanski2021rethinking,
  title={Rethinking Attention with Performers},
  author={Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xing and Gane, Alex and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kuchuk, Larry and others},
  journal={arXiv preprint arXiv:2009.14794},
  year={2021}
}

@article{dao2022flashattention,
  title={FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
  author={Dao, Tri and Fu, Daniel Y and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16344--16359},
  year={2022}
}

@article{dao2023flashattention2,
  title={FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  author={Dao, Tri},
  journal={arXiv preprint arXiv:2307.08691},
  year={2023}
}

@article{dosovitskiy2021image,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2021}
}

@article{dettmers2023qlora,
  title={QLoRA: Efficient Finetuning of Quantized LLMs},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Freitag, Luke and Zettlemoyer, Luke and Smith, Noah A},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

@inproceedings{goodfellow2016deep,
  title={Deep Learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  booktitle={MIT press},
  year={2016}
}

@inproceedings{goyal2017vqav2,
  title={Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering},
  author={Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Ferrucci, Disha and Hoffmann, Jesse},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={6904--6913},
  year={2017}
}

@article{gotmare2019closer,
  title={A Closer Look at Deep Learning Heuristics: Learning Rate Restarting, Warmup and Distillation},
  author={Gotmare, Ankit and Keskar, Nitish Shirish and Xiong, Caiming and Socher, Richard and Hoi, Steven Chu},
  journal={arXiv preprint arXiv:1812.07633},
  year={2019}
}

@article{guo2017calibration,
  title={On Calibration of Modern Neural Networks},
  author={Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
  journal={International conference on machine learning},
  pages={1321--1330},
  publisher={PMLR},
  year={2017}
}

@inproceedings{he2016deep,
  title={Deep Residual Learning for Image Recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{hendrycks2016gaussian,
  title={Gaussian Error Linear Units (GELUs)},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1606.08415},
  year={2016}
}

@article{holtzman2020curious,
  title={The Curious Case of Neural Text Degeneration},
  author={Holtzman, Ari and Buys, Janan and Du, Li and Forbes, Maxwell and Chandu, Ariel and Mansimov, Rishabh and Yih, Song},
  journal={arXiv preprint arXiv:1904.09751},
  year={2020}
}

@inproceedings{hudson2019gqa,
  title={GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering},
  author={Hudson, Drew A and Manning, Christopher D},
  booktitle={Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019}
}

@article{hu2021lora,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{kingma2015adam,
  title={Adam: A Method for Stochastic Optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2015}
}

@article{kudo2018sentencepiece,
  title={SentencePiece: A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural Text Processing},
  author={Kudo, Taku and Richardson, John},
  journal={arXiv preprint arXiv:1808.06226},
  year={2018}
}

@inproceedings{li2023blip2,
  title={BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  booktitle={International Conference on Machine Learning},
  pages={1971--1986},
  year={2023},
  organization={PMLR}
}

@inproceedings{liu2021swin,
  title={Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={10012--10022},
  year={2021}
}

@article{liu2023mmbench,
  title={MMBench: Is Your Multi-modal Model an All-around Player?},
  author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yen-Chang and Yih, Jianfeng},
  journal={arXiv preprint arXiv:2307.06281},
  year={2023}
}

@article{liu2023visual,
  title={Visual Instruction Tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yen-Chang},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

@article{loshchilov2019decoupled,
  title={Decoupled Weight Decay Regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2019}
}

@article{li2023blip2,
  title={BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  journal={International Conference on Machine Learning},
  pages={1971--1986},
  year={2023},
  organization={PMLR}
}

@inproceedings{michel2019sixteen,
  title={Are Sixteen Heads Really Better than One?},
  author={Michel, Paul and Levy, Omer and Neubig, Graham},
  booktitle={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@misc{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019},
  publisher={OpenAI}
}

@article{radford2021learning,
  title={Learning Transferable Visual Models From Natural Language Supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Biderman, Amanda and Hu, Hesha and Gokaslan, Mark and others},
  journal={International Conference on Machine Learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@inproceedings{press2017using,
  title={Using the Output Embedding to Improve Language Models},
  author={Press, Ofir and Wolf, Lior},
  booktitle={Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers},
  pages={157--162},
  year={2017}
}

@article{press2022train,
  title={Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation},
  author={Press, Ofir and Smith, Noah A and Lewis, Mike},
  journal={arXiv preprint arXiv:2108.12409},
  year={2022}
}

@inproceedings{sennrich2016neural,
  title={Neural Machine Translation of Rare Words with Subword Units},
  author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  booktitle={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1715--1725},
  year={2016}
}

@article{shazeer2020glu,
  title={GLU Variants Improve Transformer},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:2002.05202},
  year={2020}
}

@article{su2021roformer,
  title={RoFormer: Enhanced Transformer with Rotary Position Embedding},
  author={Su, Jianlin and Lu, Yu and Pan, Shengfeng and Murtadha, Ahmed and Zhao, Bo and Liu, Peng and Liu, Jiebo},
  journal={arXiv preprint arXiv:2104.09864},
  year={2021}
}

@inproceedings{touvron2021training,
  title={Training Data-Efficient Image Transformers \& Distillation through Attention},
  author={Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  booktitle={International Conference on Machine Learning},
  pages={10347--10357},
  year={2021},
  organization={PMLR}
}

@misc{touvron2023llama,
  title={Llama: Open and Efficient Foundation Language Models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Zinjewand, Faisal and others},
  year={2023},
  howpublished={\url{https://arxiv.org/abs/2302.13971}}
}

@inproceedings{vaswani2017attention,
  title={Attention is All You Need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{wang2022deepnet,
  title={DeepNet: Scaling Transformers to 1,000 Layers},
  author={Wang, Han and Chen, Zewen and Li, Haiyang and Xia, Wei and Huang, Beidou and Liang, Lili and Lu, Kang and Song, Zhihong and Zhang, Rongsheng and Xie, Wei and others},
  journal={arXiv preprint arXiv:2203.00555},
  year={2022}
}

@article{xiong2020layer,
  title={On Layer Normalization in the Transformer Architecture},
  author={Xiong, Ruibin and Yang, Yunchang and He, Di and Zheng, Kai and Zheng, Shuxin and Li, Chenxing and Zhou, Huishuai and Zhang, Yang and Liu, Yifei and Wang, Lijun and others},
  journal={International conference on machine learning},
  pages={10490--10498},
  publisher={PMLR},
  year={2020}
}

@inproceedings{yue2023mmmu,
  title={MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI},
  author={Yue, Xichen and Kotar, Young and Zhang, Kai and Li, Jeremy and Hoyle, Alicia and Rame, Andrew and Brown, Jacob and Stepleton, Lingyu and Narasimhan, Katerina and Weinberger, Kilian Q and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2023}
}

@inproceedings{devlin2018bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4171--4186},
  year={2019},
  organization={Association for Computational Linguistics}
}